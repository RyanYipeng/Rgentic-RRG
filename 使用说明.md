# ğŸ¯ MCP RAG é¡¹ç›®ä½¿ç”¨è¯´æ˜

## ğŸ“Œ æ ¸å¿ƒæ¦‚å¿µ

è¿™æ˜¯ä¸€ä¸ª RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é¡¹ç›®ï¼Œæä¾› MCP å·¥å…·æ¥å¢å¼º LLM çš„å›ç­”èƒ½åŠ›ã€‚
---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œå¯¹æ¯”æµ‹è¯•

---


### æµ‹è¯•: LlamaIndex å¯¹æ¯”æµ‹è¯• `test_llamaindex.py`

**ç‰¹ç‚¹ï¼š**
- ä½¿ç”¨ LlamaIndex çš„ Ollama LLM
- æ›´é«˜çº§çš„é›†æˆæ–¹å¼

---

## ğŸ” å¦‚ä½•ç¡®å®šå·¥å…·è¢«ä½¿ç”¨ï¼Ÿ

### æ–¹æ³•1: çœ‹å›ç­”å†…å®¹
- **çº¯æ¨¡å‹**ï¼šå›ç­”è¾ƒé€šç”¨ï¼ŒåŸºäºè®­ç»ƒæ•°æ®
- **å·¥å…·å¢å¼º**ï¼šå›ç­”æ›´å…·ä½“ï¼ŒåŒ…å«çŸ¥è¯†åº“ä¸­çš„ç²¾ç¡®ä¿¡æ¯

### æ–¹æ³•2: çœ‹ä»£ç é€»è¾‘
```python
# çº¯æ¨¡å‹ï¼ˆä¸è°ƒç”¨å·¥å…·ï¼‰
def pure_answer(question):
    prompt = f"è¯·å›ç­”ï¼š{question}"
    return llm(prompt)

# å·¥å…·å¢å¼ºï¼ˆè°ƒç”¨retriever.searchï¼‰
def rag_answer(question):
    context = retriever.search(question)  # â† è¿™é‡Œè°ƒç”¨äº†å·¥å…·ï¼
    prompt = f"åŸºäºçŸ¥è¯†åº“ï¼š{context}\né—®é¢˜ï¼š{question}"
    return llm(prompt)
```

### æ–¹æ³•3: çœ‹å¯¹æ¯”æµ‹è¯•è¾“å‡º
è¿è¡Œæµ‹è¯•åï¼Œä½ ä¼šçœ‹åˆ°ä¸¤ç§å›ç­”å¹¶æ’å±•ç¤ºï¼š
- ä¸Šæ–¹ï¼šçº¯æ¨¡å‹å›ç­”
- ä¸‹æ–¹ï¼šå·¥å…·å¢å¼ºå›ç­”
- å¯¹æ¯”ï¼šæ˜æ˜¾å·®å¼‚

---

## ğŸ’¡ å…³äº server.py

**Q: éœ€è¦è¿è¡Œ server.py å—ï¼Ÿ**

**A: çœ‹åœºæ™¯ï¼**

| åœºæ™¯ | éœ€è¦ server.py | åŸå›  |
|------|---------------|------|
| è¿è¡Œæµ‹è¯• | âŒ ä¸éœ€è¦ | æµ‹è¯•ç›´æ¥è°ƒç”¨å·¥å…·å‡½æ•° |
| æœ¬åœ°å¼€å‘ | âŒ ä¸éœ€è¦ | å¯ä»¥ç›´æ¥ import ä½¿ç”¨ |
| Cursor é›†æˆ | âœ… éœ€è¦ | Cursor é€šè¿‡ MCP åè®®è°ƒç”¨ |
| Claude Desktop | âœ… éœ€è¦ | åŒä¸Š |

**server.py çš„ä½œç”¨ï¼š**
- å°†å·¥å…·å°è£…æˆ MCP åè®®æœåŠ¡
- è®© Cursor/Claude èƒ½å¤Ÿè°ƒç”¨è¿™äº›å·¥å…·
- ä½†æµ‹è¯•æ—¶ä¸éœ€è¦ï¼Œç›´æ¥ import å°±è¡Œ

---

## ğŸ› ï¸ è‡ªå®šä¹‰å¼€å‘

### åœ¨è‡ªå·±çš„ä»£ç ä¸­ä½¿ç”¨

```python
from rag_app import Retriever, QdrantVDB, EmbedData

# åˆå§‹åŒ–
embed = EmbedData()
vdb = QdrantVDB(collection="my_collection", vector_size=embed.dim)
retriever = Retriever(vdb, embed)

# ä½¿ç”¨å·¥å…·
context = retriever.search("ä½ çš„é—®é¢˜", k=3)

# ç»“åˆ LLM
prompt = f"åŸºäºï¼š{context}\nå›ç­”ï¼šä½ çš„é—®é¢˜"
answer = your_llm(prompt)
```

### æ·»åŠ è‡ªå·±çš„æ•°æ®

ç¼–è¾‘ `rag_app/data.py`:
```python
ML_FAQ = [
    {"id": 1, "q": "é—®é¢˜1", "a": "ç­”æ¡ˆ1"},
    {"id": 2, "q": "é—®é¢˜2", "a": "ç­”æ¡ˆ2"},
    # æ·»åŠ æ›´å¤š...
]
```

---

## ğŸ“Š æ€»ç»“

| æµ‹è¯• | æŠ€æœ¯æ ˆ | é€‚ç”¨åœºæ™¯ | æ•ˆæœå±•ç¤º |
|------|--------|---------|---------|
| test_llamaindex.py | LlamaIndex + Ollama | é«˜çº§é›†æˆ | âœ… å¯¹æ¯”æ¸…æ™° |

**æ ¸å¿ƒè¦ç‚¹ï¼š**
- âœ… å¯ä»¥æ¸…æ¥šçœ‹åˆ°å·¥å…·æ˜¯å¦è¢«ä½¿ç”¨
- âœ… ä¸éœ€è¦å¯åŠ¨ server.py
- âœ… ç›´æ¥æœ¬åœ°è¿è¡Œå³å¯

---

